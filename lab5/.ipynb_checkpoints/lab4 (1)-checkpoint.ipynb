{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-15b4b12ed14d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffline\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mline_search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "import plotly.offline as py\n",
    "from plotly.graph_objs import *\n",
    "from scipy.optimize import line_search\n",
    "\n",
    "py.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "h = 0.001\n",
    "eps = 0.00001\n",
    "\n",
    "x = [1, 1, 1]\n",
    "# rate for line\n",
    "rate = 0.01\n",
    "# rate for norm\n",
    "# rate = 0.5\n",
    "\n",
    "theta = 0.5\n",
    "delta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # quadratic function for projection on a straight line method test\n",
    "    return 4*pow(x[0], 2) + pow(x[1], 2) + pow(x[2], 2)\n",
    "    # linear function for testing implemented in scipy function \n",
    "    # return x[0] + 4*x[1] + x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gradient(f, x):\n",
    "    I = np.eye(len(x))\n",
    "    return np.array([(f(x + h * I_) - f(x - h * I_))/(2.0*h) for I_ in I])\n",
    "\n",
    "def compute_gradient(f):\n",
    "    def gradients(x):\n",
    "        I = np.eye(len(x))\n",
    "        return np.array([(f(x + h*I_) - f(x - h*I_))/(2.0*h) for I_ in I])\n",
    "    return gradients\n",
    "\n",
    "def hesse_matrix(f, x):\n",
    "    I = np.eye(len(x))\n",
    "    return np.array([(gradient(f, x + h*I_) - gradient(f, x - h*I_))/(2.0*h) for I_ in I])\n",
    "    \n",
    "def next_vector_gradient(f, x, rate):\n",
    "    grads = -gradient(f, x)                                             \n",
    "    return x + rate*grads\n",
    "\n",
    "def next_vector_newton(f, x, rate):\n",
    "    summs = -np.dot(np.linalg.inv(hesse_matrix(f, x)), gradient(f, x))                                           \n",
    "    return x + rate*summs\n",
    "\n",
    "def next_vector_conjgrad(f,x,rate,hi):\n",
    "    return x+rate*hi \n",
    "\n",
    "def B_next(f,x,hi):\n",
    "    return np.sum(gradient(f,x)*np.dot(hesse_matrix(f, x),hi))/np.sum(hi*np.dot(hesse_matrix(f, x),hi))\n",
    "      \n",
    "def alpha_next(f, x, hi):\n",
    "    p0=((hesse_matrix(f, x)@x)@hi)\n",
    "    p1=((hesse_matrix(f, x)@hi)@hi)\n",
    "    p=-p0/p1\n",
    "    return p\n",
    "    \n",
    "def error(f, x, next_x):\n",
    "    args_error = np.linalg.norm(np.array(x) - np.array(next_x))\n",
    "    func_error = abs(f(next_x) - f(x))\n",
    "    grad_error = np.linalg.norm(gradient(f, next_x))\n",
    "    return args_error, func_error, grad_error\n",
    "\n",
    "def linesearch(f, x, direction):\n",
    "    old_fval = f(x)\n",
    "    gradient_f = compute_gradient(f)\n",
    "    #old_old_fval = old_fval + np.linalg.norm(gradient_f(x))/2.0\n",
    "    alpha = line_search(f, gradient_f, x, direction, old_fval=old_fval)[0]\n",
    "    return x + alpha*direction\n",
    "\n",
    "def projection_line(p):\n",
    "    a = np.zeros([3, 3])\n",
    "    a[1, 0] = 2.\n",
    "    a[1, 1] = -1.\n",
    "    a[2, 0] = 3.\n",
    "    a[2, 2] = -1.\n",
    "    a[0, 0] = 1.\n",
    "    a[0, 1] = 2.\n",
    "    a[0, 2] = 3.\n",
    "    \n",
    "    b = np.zeros(3)\n",
    "    b[1] = p[0]*2. - p[1]\n",
    "    b[2] = p[0]*3. - p[2]\n",
    "    b[0] = 1.\n",
    "    \n",
    "    res = np.linalg.solve(a, b)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def projection_norm(p):\n",
    "    fun = lambda x: pow(norm(x - p), 2)\n",
    "    cons = ({'type': 'ineq', 'fun': lambda x: -pow(x[0], 2) - 3*pow(x[1], 2) - 2*pow(x[2], 2) + 1})\n",
    "    return minimize(fun, [1., 1., 1.], method='SLSQP', constraints=cons).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \n",
    "    def __init__(self, x=x, rate=rate, theta=theta, delta=delta):\n",
    "        self.rate = rate\n",
    "        self.theta = theta\n",
    "        self.delta = delta\n",
    "        self.seq_x = [x]\n",
    "    \n",
    "    def conjgrad(self,f):\n",
    "        hi=-gradient(f, self.seq_x[-1])\n",
    "        iteration = 1\n",
    "        while True:\n",
    "            alpha = alpha_next(f, self.seq_x[-1], hi)\n",
    "            next_x = next_vector_conjgrad(f, self.seq_x[-1], alpha,hi)\n",
    "            errors = error(f, self.seq_x[-1], next_x)\n",
    "            self.seq_x.append(next_x)\n",
    "            print('{iteration} | x={x} | f(x)={f} | rate={rate} | errors=[{seq_x}, {val_f}, {grad_f}]'.format(\n",
    "                iteration=iteration, x=self.seq_x[-2], f=f(self.seq_x[-2]), rate=rate, seq_x=errors[0], val_f=errors[1], grad_f=errors[2]))\n",
    "            hi= -gradient(f, self.seq_x[-1]) +B_next(f,self.seq_x[-1],hi)*hi\n",
    "            if errors[0] < eps and errors[1] < eps and errors[2] < eps: \n",
    "                break    \n",
    "            iteration += 1\n",
    "        \n",
    "    def conjgrad_2(self,f):\n",
    "        hi = -gradient(f, self.seq_x[-1])\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            coefficient = B_next(f, self.seq_x[-1], hi)\n",
    "            next_x = linesearch(f, self.seq_x[-1], hi)\n",
    "            errors = error(f, self.seq_x[-1], next_x)\n",
    "            self.seq_x.append(next_x)\n",
    "            print('{iteration} | x={x} | f(x)={f} | rate={rate} | errors=[{seq_x}, {val_f}, {grad_f}]'.format(\n",
    "                iteration=iteration, x=self.seq_x[-2], f=f(self.seq_x[-2]), rate=rate, seq_x=errors[0], val_f=errors[1], grad_f=errors[2]))\n",
    "            hi = -gradient(f, self.seq_x[-1]) + B_next(f, self.seq_x[-1], hi)*hi\n",
    "            if errors[0] < eps and errors[1] < eps and errors[2] < eps: \n",
    "                break    \n",
    "            iteration += 1\n",
    "    \n",
    "    def gradient_method(self, f):\n",
    "        iteration = 1\n",
    "        while True:\n",
    "            next_x = next_vector_gradient(f, self.seq_x[-1], self.rate)\n",
    "            rate = self.rate\n",
    "            while f(next_x) - f(self.seq_x[-1]) > -self.theta*rate*np.linalg.norm(gradient(f, self.seq_x[-1]))**2: \n",
    "                rate = rate*self.delta\n",
    "                next_x = next_vector_gradient(f, self.seq_x[-1], rate)\n",
    "\n",
    "            errors = error(f, self.seq_x[-1], next_x)\n",
    "            self.seq_x.append(next_x)\n",
    "            print('{iteration} | x={x} | f(x)={f} | rate={rate} | errors=[{seq_x}, {val_f}, {grad_f}]'.format(\n",
    "                iteration=iteration, x=self.seq_x[-2], f=f(self.seq_x[-2]), rate=rate, seq_x=errors[0], val_f=errors[1], grad_f=errors[2]))\n",
    "            if errors[0] < eps and errors[1] < eps and errors[2] < eps: \n",
    "                break\n",
    "\n",
    "            iteration += 1\n",
    "            \n",
    "    def gradient_projection_method(self, f):\n",
    "        iteration = 1\n",
    "        while True:\n",
    "            next_x = projection_line(next_vector_gradient(f, self.seq_x[-1], self.rate))\n",
    "            # next_x = projection_norm(next_vector_gradient(f, self.seq_x[-1], self.rate))\n",
    "            rate = self.rate\n",
    "            \n",
    "            errors = error(f, self.seq_x[-1], next_x)\n",
    "            self.seq_x.append(next_x)\n",
    "            print('{iteration} | x={x} | f(x)={f} | rate={rate} | errors=[{seq_x}, {val_f}, {grad_f}]'.format(\n",
    "                iteration=iteration, x=self.seq_x[-2], f=f(self.seq_x[-2]), rate=rate, seq_x=errors[0], val_f=errors[1], grad_f=errors[2]))\n",
    "            if errors[0] < eps and errors[1] < eps: \n",
    "                break\n",
    "\n",
    "            iteration += 1\n",
    "    \n",
    "    def newton_method(self, f):\n",
    "        iteration = 1\n",
    "        \n",
    "        while True:\n",
    "            next_x = next_vector_newton(f, self.seq_x[-1], self.rate)\n",
    "            rate = self.rate\n",
    "            while f(next_x) - f(self.seq_x[-1]) > -self.theta*rate*np.dot(gradient(f,self.seq_x[-1]), -np.dot(np.linalg.inv(hesse_matrix(f, self.seq_x[-1])), gradient(f, self.seq_x[-1]))): \n",
    "                rate = rate*self.delta\n",
    "                next_x = next_vector_newton(f, self.seq_x[-1], rate)\n",
    "                \n",
    "            errors = error(f, self.seq_x[-1], next_x)\n",
    "            self.seq_x.append(next_x)\n",
    "            print('{iteration} | x={x} | f(x)={f} | rate={rate} | errors=[{seq_x}, {val_f}, {grad_f}]'.format(\n",
    "                iteration=iteration, x=self.seq_x[-2], f=f(self.seq_x[-2]), rate=rate, seq_x=errors[0], val_f=errors[1], grad_f=errors[2]))\n",
    "            if errors[0] < eps and errors[1] < eps and errors[2] < eps : \n",
    "                break\n",
    "\n",
    "            iteration += 1\n",
    "    \n",
    "    def plot_3d(self, f):\n",
    "        seq_x = np.array(self.seq_x)\n",
    "        x, y = seq_x.T\n",
    "        axis_x = np.arange(np.min(x) - 2, np.max(x) + 2, 0.05)\n",
    "        axis_y = np.arange(np.min(y) - 2, np.max(y) + 2, 0.05)\n",
    "            \n",
    "        f_values = np.array([[f([_x, _y]) for _y in axis_y] for _x in axis_x])\n",
    "        curve_values = np.array([f([_x, _y]) for _x, _y in zip(x, y)])\n",
    "                  \n",
    "        trace = Scatter3d(\n",
    "            mode='lines+markers',\n",
    "            x=x, y=y, z=curve_values,\n",
    "            marker=dict(size=7, color=curve_values, colorscale='Viridis',),\n",
    "            line=dict(color='black', width=4)\n",
    "        )\n",
    "    \n",
    "        data = Data([Surface(x=axis_x, y=axis_y, z=f_values), trace])\n",
    "    \n",
    "        layout = dict(\n",
    "            title='Surface f(x)',\n",
    "            autosize=False,\n",
    "            width=500,\n",
    "            height=500,\n",
    "            margin=dict(l=65, r=20, b=65, t=90)\n",
    "        )\n",
    "\n",
    "        figure = dict(data=data,layout=layout)\n",
    "        py.iplot(figure,filename='3dsurface')\n",
    "        \n",
    "    def plot_2d(self, f):\n",
    "        seq_x = np.concatenate(self.seq_x)\n",
    "        axis_x = np.arange(np.min(seq_x) - 2, np.max(seq_x) + 2, 0.05)\n",
    "        f_values = np.array([f([_x]) for _x in axis_x])\n",
    "        curve_values = np.array([f([_x]) for _x in seq_x])\n",
    "        \n",
    "        trace = Scatter(\n",
    "            mode='lines+markers',\n",
    "            x=seq_x, y=curve_values,\n",
    "            marker=dict(size=7, color=curve_values, colorscale='Viridis',),\n",
    "            line=dict(color='black', width=4)\n",
    "        )\n",
    "        data = Data([Scatter(x=axis_x, y=f_values), Scatter(x=seq_x, y=curve_values)])\n",
    "        \n",
    "        layout = dict(\n",
    "            title='Surface f(x)',\n",
    "            autosize=False,\n",
    "            width=500,\n",
    "            height=500,\n",
    "            margin=dict(l=65, r=20, b=65, t=90)\n",
    "        )\n",
    "\n",
    "        figure = dict(data=data,layout=layout)\n",
    "        py.iplot(figure,filename='2dsurface')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "optimizer = Optimizer()\n",
    "optimizer.conjgrad_2(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
